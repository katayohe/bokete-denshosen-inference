{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hosting a Boke AI Keras Model on Amazon SageMaker\n",
    "\n",
    "Amazon SageMaker is a fully-managed service that provides developers and data scientists with the ability to build, train, and deploy machine learning (ML) models quickly. Amazon SageMaker removes the heavy lifting from each step of the machine learning process to make it easier to develop high-quality models. The SageMaker Python SDK makes it easy to train and deploy models in Amazon SageMaker with several different machine learning and deep learning frameworks, including TensorFlow and Keras.\n",
    "\n",
    "In this notebook, we train and host a Keras boke-AI model on SageMaker. The model used for this notebook is a neural network (CNN and Bidirectional-LSTM) for image captioning that was developed by Ryuichi Ishikawa (Dentsu Digital).\n",
    "\n",
    "<br>\n",
    "\n",
    "in the `us-west-2`, `us-east-1`, `us-east-2`, and `ap-northeast-1` regions. \n",
    "\n",
    "Price per hour depends on your region and instance type. You can reference prices on the [SageMaker pricing page](https://aws.amazon.com/sagemaker/pricing/).  \n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "First, we define a few variables that are be needed later in the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "from sagemaker import Session\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = Session()\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#　学習済みを利用\n",
    "preprocessed_bokekan_metadata = 'xxxx.cloudfront.net/xx..xx/bokekan_metadata'\n",
    "trained_model_path = 'xxxxx.cloudfront.net/xx..xx/model.tar.gz'\n",
    "\n",
    "s3_folder_name = \"hoge\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bokete Bokekan dataset\n",
    "\n",
    "Bokete is one of the most popular commedy web site. [Bokekan dataset](../README.md) consists of 1M+ images/text pairs to 4 different classes. Here are the classes in the dataset, as well as 1 random image/text pair:\n",
    "\n",
    "| class | boke (image/text pair) | number of stars |\n",
    "| ---- | ----: | ---- |\n",
    "| blue | 98,736 | 0 |\n",
    "| yellow | 955,901 | 1 - 100 |\n",
    "| green | 37,342 | 101 - 1000 |\n",
    "| red | 8,183 | 1001 - 10000 |\n",
    "| sp | 380 | 10001+ |\n",
    "| Total | 1,100,542 | boke |\n",
    "\n",
    "<div align=\"center\"><img src=\"https://d1.awsstatic.com/Developer%20Marketing/jp/magazine/2020/img_bokete_01.a9c39e30940cf3c6fb5ba795b9202a57a843da05.jpg\" alt=\"bokete\" width=\"320\"/></div>\n",
    "<div align=\"center\">(Photo by IHA Central Office, licensed under the Creative Commons Attribution License 2.0)</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you will copy the Bokekan dataset to your SageMaker default bucket. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the trained model\n",
    "\n",
    "After we train our model, we can deploy it to a SageMaker Endpoint, which serves prediction requests in real-time. To do so, we simply call `deploy()` on our estimator, passing in the desired number of instances and instance type for the endpoint.\n",
    "\n",
    "Because we're using TensorFlow Serving for deployment, our training script saves the model in TensorFlow's SavedModel format.  \n",
    "\n",
    "We don't need accelerated computing power for inference, so let's switch over to a <b>ml.m4.xlarge</b> instance type. \n",
    "\n",
    "For more information about deploying Keras and TensorFlow models in SageMaker, see [this blog post](https://aws.amazon.com/blogs/machine-learning/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve parameters\n",
    "!wget {preprocessed_bokekan_metadata}/param.json ./\n",
    "!wget {trained_model_path} ./\n",
    "\n",
    "# upload model file to my bucket\n",
    "model_path = \"s3://\"+default_bucket+\"/\"+s3_folder_name+\"/\"\n",
    "!aws s3 cp ./model.tar.gz {model_path}\n",
    "\n",
    "with open('./param.json') as json_file:\n",
    "    param = json.load(json_file)\n",
    "    print(param)\n",
    "\n",
    "max_word = param['max_word']\n",
    "vocabulary_size = param['vocabulary_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlowModel\n",
    "\n",
    "model = TensorFlowModel(\n",
    "    model_data=model_path,\n",
    "    role=role,\n",
    "    framework_version='2.3'\n",
    "    )\n",
    "\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type='local')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke the endpoint\n",
    "\n",
    "To verify the that the endpoint is in service, we generate some random data in the correct shape and get a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow import reshape, stack, convert_to_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the test dataset for predictions. We will setup for inference preprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models              import Model\n",
    "from tensorflow.keras.applications.vgg16  import VGG16 \n",
    "\n",
    "VGG = VGG16(weights='imagenet')\n",
    "VGG._layers.pop()\n",
    "VGG = Model(inputs=VGG.inputs, outputs=VGG.layers[-1].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dictionary\n",
    "id_word = pd.read_csv(\"https://\"+os.path.join(preprocessed_bokekan_metadata, 'id_word_bokete_data.csv'), index_col=0).to_dict()['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample the first word from a distribution\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype(\"float32\")\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probs = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find your favorite image from https://bokete.jp/boke/select, then copy&paste the image url. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = 'https://d2dcan0armyq93.cloudfront.net/photo/odai/600/2182ab815bdb2abae3b67e723861ac08_600.jpg'\n",
    "# image_url = 'https://d2dcan0armyq93.cloudfront.net/photo/odai/600/0235436e0d5d54c882f485e367f68fe6_600.jpg'\n",
    "# image_url = 'https://d2dcan0armyq93.cloudfront.net/photo/odai/600/db6e0eb2dd26330e1884d0812d78e783_600.jpg'\n",
    "\n",
    "!wget {image_url} ./\n",
    "image_path = image_url.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess test image array\n",
    "img = load_img(image_path, target_size=(224, 224))\n",
    "x = np.array(img)/255\n",
    "\n",
    "x = VGG(reshape(x, [1, 224, 224, 3]))\n",
    "X_test_image = np.array(x)\n",
    "\n",
    "# Prepare empty text array\n",
    "x2 = np.zeros([1, max_word])\n",
    "x2[0, -1] = 1. # set {1: 'Beginning Of Sentence (BOS)'} at the beginning \n",
    "X_test_text = np.array(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data loaded, we can use it for predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "max_length = max_word\n",
    "for i in range(max_length):\n",
    "    payload = {\"inputs\": {\"image\": X_test_image.tolist(), \"text\": X_test_text.tolist()}}\n",
    "    outputs = np.array(predictor.predict(payload)['outputs'])\n",
    "\n",
    "    # sampling the initial word\n",
    "    if i == 0:\n",
    "        X_test_text = np.append(X_test_text, sample(outputs[0], 1.0))[1:].reshape(1,-1)\n",
    "\n",
    "    elif outputs.argmax() == 0:\n",
    "        break\n",
    "\n",
    "    else:\n",
    "        X_test_text = np.append(X_test_text, outputs.argmax())[1:].reshape(1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the predictions, we calculate our model accuracy and create a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "boke = ''.join(itemgetter(*X_test_text[0].tolist())(id_word)).split('BOS')[-1]\n",
    "\n",
    "plt.imshow(img)\n",
    "print(boke) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aided by the colors of the heatmap, we can use this confusion matrix to understand how well the model performed for each label.print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "delete folder from default bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 rm s3://{default_bucket}/{s3_folder_name} --recursive"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "notice": "Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
